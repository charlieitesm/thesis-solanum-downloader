{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Roadmap to Thesis and Notes","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPpgU4Me2TBrPbx0Oq0wb7J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LkT-rBgY1bHC"},"source":["# 1. What has been done"]},{"cell_type":"markdown","metadata":{"id":"Kxv1qxa3uGNC"},"source":["## 1. DownloadingDatasets-Idigbio\n","  1. Processing of the CSV files with metadata were processed using Pandas\n","  1. 241 of the records in Idigbio haven't been identified, we can use these as part of the test dataset\n","  1. There are 6498 records that have been identified.\n","  1. Some typos were found on some of the records and these were fixed\n","  1. As of 2021, there are 134 identified species in the Solanum Genus for Mexico\n","  1. The dataset has 202 unique species names, but this happens because some names are invalid/outdated/have been replaced/are outright misidentified\n","  1. A mapping was created to replace the misidentified or incorrect species name\n","  1. We got the value counts per species and proceeded to merge it with the multimedia dataframe. Remember that the ocurrences and multimedia mappings are downloaded from 2 differente sources.\n","  1. There are multiple pictures per coreid and don't have duplicated URLS\n","  1. We found that we are missing some mexican species:\n","    * atitlanum\n","    * aviculare\n","    * bicorne\n","    * caripense\n","    * davisense\n","    * edmundoi\n","    * guerreroense\n","    * knoblochii\n","    * nitidibaccatum\n","    * setigeroides\n","    * triunfense\n","    \n","    But this is because they are very rare species\n","  1. The solanum section to which each species belongs is not present in the original iDigBio dataframe, it needs to be aggregated based on the feedback of Gera.\n","  1. There are 6476 records in this database\n","  1. With this, we are able to add the information of the section to each of the rows for the pictures.\n","    * There are some underrepresented sections, but this is normal as they are quite rate\n","    * There are no examples of the **archaesolanum** section in this dataset.\n","  1. The final result was downloaded to `idigbio_images_by_sections.csv`"]},{"cell_type":"markdown","metadata":{"id":"d88Yqno61ijB"},"source":["## DownloadingDatasets-Gbif\n","\n","1. The same treatment was done for gbif as for idigbio, but in this one, there are missing 10 species:\n","  * Solanum Atitlanum\n","  * Solanum Aviculare\n","  * Solanum Bicorne\n","  * Solanum Davisense\n","  * Solanum Deflexum\n","  * Solanum Edmundoi\n","  * Solanum Nitidibaccatum\n","  * Solanum Rostratum\n","  * Solanum Rudepannum\n","  * Solanum Setigeroides\n","1. A correction has been done to some of the records to have the species updated or corrected\n","1. There are 6483 records and 6726 after removing null values with no pictures\n","1. While searching for duplicated URLs, it was found that there are some pictures that contain more than one specimen on the same sheet, but given that both species belong to the same section, we should be OK to keep them.\n","1. We are missing some mexican species:\n","  1. solanum atitlanum\n","  1. solanum aviculare\n","  1. solanum bicorne\n","  1. solanum davisense\n","  1. solanum deflexum\n","  1. solanum edmundoi\n","  1. solanum nitidibaccatum\n","  1. solanum rostratum\n","  1. solanum rudepannum\n","  1. solanum setigeroides\n","\n","1. We add the section to the rows of each of the pictures and find that we don't have specimens of the `archaesolanum` section which is a really rare sectin.\n","1. Results are saved to `gbif_images_by_sections.csv`"]},{"cell_type":"markdown","metadata":{"id":"Ml7NAYZq4mVC"},"source":["## DownloadingAllDatasets\n","1. This processes the 2 CSV files from gbif and idigbio, merges them, remove the duplicated URLs and checks how many actual valid records there are, however, it doesn't actually download anything, it's just mining for information.\n","1. According to these experiments, there are 9292 unique URLs between Idigbio and Gbif to download."]},{"cell_type":"markdown","metadata":{"id":"FS0GA1867ymh"},"source":["## Dataset downloader\n","It's a Python program designed to download and categorize images parsed from the CSV files above.\n","\n","It:\n","1. Filters out duplicated URls so as to try them only once\n","1. Creates a parent download folder\n","1. Downloads the images to a solanum section folder\n","\n","The code is hosted in a Git repository in Github `charlieitesm/tesis-dataset-downloader`.\n","\n","It uses BS4, NP, Pandas and Requests in order to download everything. A report of those URLs from which it was not possible to perform a download is written to `failed_images.csv`\n"]},{"cell_type":"markdown","metadata":{"id":"pIYdjnxNdtCw"},"source":["## Solanum Dataset Metrics\n","\n","1. We want to obtain the metrics for the dataset:\n","  1. Section to which it belongs\n","  1. Species\n","  1. Source of file\n","  1. Size (MB)\n","  1. Resolution\n","  1. Type of file\n","  1. Hash (fingerprint)\n","  \n","  For this I used `ImageHash` to calculate the fingerprint of the images in order to identify near-duplicatres.\n","1. Originally, 8937 files where processed from the filesystem (this means that between this and the 9292 we had on record, means that the difference is because of the images that we were not able to download.\n","1. There are 190 near-duplicates out of 92 unique images, we say that a duplicate are one such that they have the same: \n","  * Hash, section, species, filesize_mb\n","1. Using a flag on the dataset of duplicates, we can remove the records we don't want, specially those that have smaller sizes.\n","1. With the help of a biologist, I pruned those files that have the same fingerprint, but are classified in different sections.\n","1. Results were saved to `Downloaded_dedup_images_report.csv`\n"]},{"cell_type":"markdown","metadata":{"id":"LyQ0UUkhuNv5"},"source":["# Important files\n","\n","1. idigbio_images_by_sections.csv\n","  * Contains all of the single images with the species, the section and the URL of the media\n","1. gbif_images_by_sections\n","  * Contains the single images with the species, URL and section of the section.\n","1. Downloaded_dedup_images_report.csv\n","  * Contains the dataframe with the images that will be kept before preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"RZkDeOGvkSOn"},"source":["# Plan of attack\n","\n","## Enero\n","1. ~Medir cuántas imágenes tienen una resolución menor a 512 y a qué sección pertenecen. Ver si es factible removerlas~\n","1. ~Quitar todas las secciones que tengan menos de 100 ejemplares.~\n","1. ~Reducir a 512x512 la resolucion de las imagenes~\n","1. ~Comprimir todo el dataset~\n","1. Subirlo a la DGX-1\n","1. Entrenamiento de modelos\n","    * Para todos los modelos se debe:\n","        * Guardar modelo en binario\n","        * Metricas\n","        * Usar CV=5\n","        * Usar data augmentation con reflexion, escalamiento, sheer\n","    1. Usar una VGG8 o VGG16 implementada a mano\n","\n","## Febrero\n","1. Continuacion entrenamiento de modelos\n","    1. Usar una VGG mas grande de libreria\n","    1. Repetir para ResNet50\n","    1. Escoger una arquitectura mas grande y compleja (state of the art)\n","1. Decidir si se usara Transfer Learning con ImageNet\n","\n","## Marzo - Abril\n","1. Escribir tesis"]},{"cell_type":"markdown","metadata":{"id":"VkWdAc1i2L1r"},"source":["# Interesting questions and issues to pursue\n","\n","1. Measure how outdated are these records in both iDigBio and Gbif, in particular how many records can be updated with new values for the species.\n","1. Measure what database has the most unique files and how many repeated records there are in each of the database.\n","  * What database seems to be the most reliable with more unique records and less errors?"]},{"cell_type":"markdown","metadata":{"id":"785KG8-GuRbZ"},"source":["# Notes\n","\n","## Questions\n","1.\n","\n","---\n","\n","## Advice\n","1. Focus on sections that have at least 100 samples\n","1. About the size, there's no standard, check the state of the art, consider 512x512\n","    1. Checar cuales imagenes tienen menos de 512 y ver si podemos quitarlas del dataset\n","1. Sobre arquitectura de CNN\n","    1. Primero hay que ver que pasa sin usar Transfer Learning\n","    1. Usar una VGG-8/16 para ver que tan bien se comporta, sus metricas\n","        1. Hacerla a mano primero antes de usar una mejor\n","    1. Moverse a usar VGG mas grande y ResNet50\n","    1. Comparar con una mas grande y compleja\n","    1. Comparar con Transfer Learning\n","1. GradCam\n","    1. Una vez clasificada ilumina y resalta lo que esta usando para hacer la clasificacion, esto podemos dejarlo hasta el final.\n","\n","1. Empieza para mas facil\n","    1. Vgg16 de libreria sin TL, enfocarse a comparar diferentes arquitecturas\n","    1. De ahi nos pasamos a Vgg8 a mano\n","    1. De ahi a algo mas complejo\n","    1. Y luego ver si despues nos alcanza TL\n","    1. Usar imagenes de 512x512 y usar data augmentation, reflexion, escalamiento"]}]}